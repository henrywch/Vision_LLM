# -----------------
# Model Parameters
# -----------------

model_name_or_path: ../../../models/qwen/qwen2.5-0.5b
visual_tower: ../../../models/clip/clip-vit-large-patch14-336
template: qwen

# -----------------
# Method Parameters
# -----------------

stage: pt
do_train: true
finetuning_type: lora
lora_target: all
shift_attn: false
use_longlora: false

# --------------------
# Dataset Parameters
# --------------------

dataset: llava_v1_5_pretrain

# -------------------
# Output Parameters
# -------------------

output_dir: ../../checkpoints/qwen2-0.5b-llava-pretrain
logging_steps: 10
save_steps: 1000
report_to: tensorboard
# plot_loss: true

# -----------------
# Train Parameters
# -----------------

per_device_train_batch_size: 8
gradient_accumulation_steps: 4
learning_rate: 1.0e-3
num_train_epochs: 1.0
lr_scheduler_type: cosine
fp16: true
use_unsloth: false
